{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Soumi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Soumi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Soumi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Soumi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\Soumi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     C:\\Users\\Soumi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import feedparser as fp\n",
    "import json\n",
    "import newspaper             #For scrapping articles      \n",
    "from newspaper import Article \n",
    "from time import mktime\n",
    "from datetime import datetime\n",
    "import spacy                 #Library for performing NER\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import nltk\n",
    "import re\n",
    "from rouge import Rouge       # Library for evaluating summary \n",
    "from collections import Counter\n",
    "from geotext import GeoText   # Location extraction\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.downloader.download('maxent_ne_chunker')\n",
    "nltk.downloader.download('words')\n",
    "nltk.downloader.download('treebank')\n",
    "nltk.downloader.download('maxent_treebank_pos_tagger')\n",
    "import string\n",
    "from gensim.summarization.summarizer import summarize\n",
    "\n",
    "from sumy.parsers.plaintext import PlaintextParser #We're choosing a plaintext parser\n",
    "from sumy.nlp.tokenizers import Tokenizer \n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer #We're choosing Lexrank, other algorithms are also built in\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "\n",
    "import articleDateExtractor\n",
    "\n",
    "\n",
    "global str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Soumi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas as pd, numpy, textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "import spacy\n",
    "import re\n",
    "import csv\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from  sklearn.metrics  import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = nltk.WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess(rawText):\n",
    "    cleanText = re.sub(r'[^\\w\\s]','',rawText.lower())\n",
    "    tokens = word_tokenize(cleanText)\n",
    "    tokens = [token for token in tokens if not token in stop_words]\n",
    "    tokens = [lemma.lemmatize(token) for token in tokens ]\n",
    "    processedText = (\" \".join(tokens))\n",
    "    return processedText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(r'C:/AIR/AIR Final/AIR/AIR/air-20190514T004703Z-001/air/indiaCorpus_01_04.csv')\n",
    "\n",
    "target, notes = [], []\n",
    "reader = csv.reader(data, delimiter=\",\")\n",
    "next(reader, None)\n",
    "for i, line in enumerate(reader):\n",
    "    target.append(line[1])\n",
    "    rawText = line[0]\n",
    "    notes.append(preprocess(rawText))\n",
    "\n",
    "dataSet = pd.DataFrame()\n",
    "dataSet['notes'] = notes\n",
    "dataSet['target'] = target\n",
    "dataSet = dataSet.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(dataSet['notes'], dataSet['target'])\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "vector = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "vector.fit(dataSet['notes'])\n",
    "train_x =  vector.transform(train_x)\n",
    "valid_x =  vector.transform(valid_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naieve Bayes Event Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8437299035369775\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB().fit(train_x, train_y)\n",
    "predicted = clf.predict(valid_x)\n",
    "print(accuracy_score(valid_y,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Event Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9504823151125402"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "SVM = svm.LinearSVC()\n",
    "SVM.fit(train_x, train_y)\n",
    "predict = SVM.predict(valid_x)\n",
    "accuracy_score(valid_y,predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Event Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9028938906752412"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=10)\n",
    "rf.fit(train_x, train_y)\n",
    "predict = rf.predict(valid_x)\n",
    "accuracy_score(valid_y,predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(r'C:/AIR/AIR Final/AIR/AIR/air-20190514T004703Z-001/air/indiaActorsCorpus2_2010_2019.csv')\n",
    "\n",
    "target, notes = [], []\n",
    "reader = csv.reader(data, delimiter=\",\")\n",
    "next(reader, None)\n",
    "for i, line in enumerate(reader):\n",
    "    target.append(line[1])\n",
    "    rawText = line[0]\n",
    "    notes.append(preprocess(rawText))\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['notes'] = notes\n",
    "df['target'] = target\n",
    "df['target'] = df['target'].replace({'CPI (Maoist): Communist Party of India (Maoist)':'CPI'})\n",
    "df['target'] = df['target'].replace({'BJP: Bharatiya Janata Party':'BJP'})\n",
    "df['target'] = df['target'].replace({'INC: Indian National Congress':'INC'})\n",
    "df['target'] = df['target'].replace({'Protesters':'Protesters'})\n",
    "df['target'] = df['target'].replace({'Rioters':'Rioters'})\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "actrain_x, acvalid_x, actrain_y, acvalid_y = model_selection.train_test_split(df['notes'], df['target'])\n",
    "\n",
    "acencoder = preprocessing.LabelEncoder()\n",
    "actrain_y = acencoder.fit_transform(actrain_y)\n",
    "acvalid_y = acencoder.fit_transform(acvalid_y)\n",
    "\n",
    "acvector = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=3000)\n",
    "acvector.fit(df['notes'])\n",
    "actrain_x =  acvector.transform(actrain_x)\n",
    "acvalid_x =  acvector.transform(acvalid_x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Actor Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.018825183565067957"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "actorRf = RandomForestClassifier(n_estimators=50)\n",
    "actorRf.fit(actrain_x, actrain_y)\n",
    "acpredict = actorRf.predict(acvalid_x)\n",
    "accuracy_score(acvalid_y,acpredict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Actor Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015075769411029527"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "acSVM = svm.LinearSVC()\n",
    "acSVM.fit(actrain_x, actrain_y)\n",
    "predict = acSVM.predict(acvalid_x)\n",
    "accuracy_score(acvalid_y,predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TOI': {'rss': 'https://timesofindia.indiatimes.com/rssfeeds/-2128936835.cms',\n",
       "  'link': 'https://timesofindia.indiatimes.com/elections/news'},\n",
       " 'Hindustan Times': {'rss': 'https://www.hindustantimes.com/rss/india/rssfeed.xml',\n",
       "  'link': 'https://www.hindustantimes.com/'},\n",
       " 'India Today': {'rss': 'https://www.indiatoday.in/rss/1206578',\n",
       "  'link': 'https://indianexpress.com/?s=politics'},\n",
       " 'Business Standard': {'rss': 'https://www.business-standard.com/rss/latest.rss',\n",
       "  'link': 'https://www.business-standard.com/lok-sabha-elections-2019'}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('newspaperLinks.json') as file:\n",
    "    sources = json.load(file)\n",
    "sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizeText(text):\n",
    "    string=text\n",
    "    parser = PlaintextParser.from_string(string, Tokenizer(\"english\"))\n",
    "\n",
    "    summarizer = LexRankSummarizer()\n",
    "    summary_lex = summarizer(parser.document, 1) #Summarize the document with 5 sentences\n",
    "    summary_1=''\n",
    "    for line in summary_lex:\n",
    "        summary_1+=str(line)\n",
    "    return summary_1\n",
    "\n",
    "def getLocation(ents, text):\n",
    "    loc = \"\"\n",
    "    items = [x.text for x in ents if x.label_ in 'LOC']\n",
    "    loc_ = [Counter(items).most_common(1)]\n",
    "    places = GeoText(text)\n",
    "    topPlace = [Counter(places.cities).most_common(1)]\n",
    "    \n",
    "    places = (places.cities)\n",
    "    locList = [i for i in places if i in items]\n",
    "    total = items+locList \n",
    "    location_ = [Counter(total).most_common(1)]\n",
    "    if len(location_) == 0:\n",
    "        if(len(loc_))!=0:\n",
    "            loc = loc_[0][0][0]\n",
    "        elif (len(topPlace)!=0):\n",
    "            loc = topPlace[0]\n",
    "        else:\n",
    "            loc = \"\"\n",
    "    else:\n",
    "        if(len(location_))!=0:\n",
    "            if(len(location_[0]))!=0:\n",
    "                if(len(location_[0][0]))!=0:\n",
    "                    loc = location_[0][0][0]\n",
    "        else:\n",
    "            loc = \"unknown\"\n",
    "    return loc\n",
    "\n",
    "def getEventType(tokens):\n",
    "    eventType = \"other\"\n",
    "    for event, types in eventTypes.items():\n",
    "        tag = [token for token in tokens if any(typ in token for typ in types)]\n",
    "        if len(tag) != 0:\n",
    "            eventType = event\n",
    "            break\n",
    "    return eventType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Scraper and Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOI\n",
      "Lok Sabha elections: The Indian 'Miss Marple' snooping on election candidates\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "Strong polling in areas dominated by Muslims could influence results in Delhi\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "SC junks plea to advance voting time for Ramzan\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "PM Modi blames Congress for China pipping India in defence production\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "Savarkar no longer ‘Veer’ in Rajasthan textbook\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "Hindustan Times\n",
      "In Mayawati’s ‘sinking ship’ jibe at Modi, a claim about RSS’ stand on PM\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "Lok Sabha elections 2019 live updates: PM Modi to shortly address poll rally in Uttar Pradesh’s Ballia\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "Lok Sabha Elections 2019: Bad days for BJP will begin after May 23, says Mayawati\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "Lok Sabha elections 2019: Priyanka Gandhi mocks PM Modi’s cloud theory, says he’s on people’s radar\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "Lok Sabha elections 2019: KCR pushes for national role in post-poll scenario\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "Shah alleges Didi made Bengal ‘Kangal’, TMC calls him ‘low life’\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "‘You should be ashamed… why scold mentor over riot remark?’ PM Narendra Modi stings Rahul Gandhi\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "Did Stalin snub KCR’s Federal Front pitch? Just a ‘courtesy call’, says DMK\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "Confident of winning AP polls, Jagan moves Hyderabad office to Amaravati\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "India Today\n",
      "Bloody hell, says Maharashtra Congress MLA as she abuses babus at a meeting | WATCH\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "Man wanted in murder case arrested soon after stepping out of polling booth in Delhi\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "Report on pollution to decide future of industries in Agra\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "It’s tradition for a new govt to form panel to review syllabus: CM Ashok Gehlot on Veer Savarkar row\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "PM Modi curtails tenure of senior railway officer who sought complimentary IPL passes\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "Priyanka Gandhi gets private jet to rush terminally ill girl to AIIMS\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "WATCH: Telangana Congress leaders exchange blows while protesting against KCR govt\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "Supreme Court reserves order in Rafale review, contempt case against Rahul Gandhi\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "BJP demands CBI probe into Alwar gangrape case\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "Alwar gangrape case: All six accused arrested, BJP holds protests\n",
      "['Riots']\n",
      "bad input shape ()\n",
      "BJP respects Rajiv Gandhi, but has right to question his govt: Nirmala Sitharaman\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "Rajiv Gandhi assassination case: SC dismisses pleas opposing Tamil Nadu govt's move to release convicts\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "BJP's latest on Rajiv Gandhi: Instruction to kill came from his office in 1984 riots\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "ED team in London to oppose Nirav Modi's bail application\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "Alwar gangrape takes political turn, BJP alleges Gehlot govt suppressed case due to polls\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "Woman gangraped in Rajasthan: Alwar SP removed, BJP calls it worse than Nirbhaya case\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "Rajiv Gandhi assassination case: Nalini approaches high court against TN governor\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "2 choke to death while cleaning septic tank in Delhi's Rohini, 3 hospitalised\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "In RTI reply, Centre says no records of surgical strikes during UPA regime\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "Rafale and Rahul Gandhi: Curious case of missing dates in Supreme Court\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "BJP leaders should say I am Pagal, not I am Chowkidar: Siddaramaiah\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "Modi blames Mamata's arrogance for failed Cyclone Fani meet, Didi says don't need expiry PM\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "PM conducts aerial survey of areas affected by Fani in Odisha\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "PM Modi praises Odisha CM Naveen Patnaik on tackling Cyclone Fani\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "Cyclone Fani at centre of storm: PM Modi called Mamata Banerjee but calls not returned, says Centre\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "BJP leader shot dead by terrorists in J&K's Nowgam\n",
      "['Violence against civilians']\n",
      "bad input shape ()\n",
      "BJP candidate from West Bengal injured in road accident\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "Business Standard\n",
      "Election 2019 may see the highest voter turnout, but will it benefit BJP?\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "Who's with whom? Parties seek partners as Lok Sabha election nears end\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "Lok Sabha elections 2019: The rise and rise of the rational Indian voter\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "Lok Sabha polls 2019 LIVE: Enough is enough, says Modi on grand alliance\n",
      "['Protests']\n",
      "bad input shape ()\n",
      "Priyanka Gandhi takes jibe at PM Modi for his 'cloudy weather' theory\n",
      "['Protests']\n",
      "bad input shape ()\n"
     ]
    }
   ],
   "source": [
    "collections = {\n",
    "    'total' : []\n",
    "}\n",
    "from time import mktime\n",
    "from datetime import datetime\n",
    "from lexrank import STOPWORDS, LexRank\n",
    "\n",
    "event_type = \"\"\n",
    "data = {}\n",
    "data['newspapers'] = {}\n",
    "articlesData ={}\n",
    "collectedNews = {}\n",
    "events = ['arrest','arrested','violence against civilians','rioting','protesting','riots','protest','battles','violence','remote violence','assault','attack','bloodshed','brutality','clash','confusion','cruelty','disorder','disturbance','fighting','rampage','struggle','terrorism','abandon','acuteness','bestiality','blowup','coercion','compulsion','constraint','destructiveness','duress','ferocity','fervor','fierceness','flap','frenzy','fury','fuss','harshness','murderousness','onslaught','passion','power','roughness','ruckus','rumble','savagery','scam','severity','sharpness','storm','storminess','tumult','turbulence','uproar','vehemence','wildness','brute','force','foul','play','raging','anarchy','brawl','disturbance','lawlessness','protest','storm','strife','trouble','turbulence','turmoil','uproar','anarchism','burst','commotion','confusion','distemper','flap','fray','free-for-all','fuss','hassle','misrule','mix-up','quarrel','racket','row','ruckus','ruction','rumble','rumpus','run-in','scene','shivaree','shower','snarl','stir','to-do','tumult','brannigan','mob','violence','street','fighting','wingding','action','assault','attack','bloodshed','bombing','campaign','clash','combat','conflict','crusade','encounter','fighting','hostility','skirmish','strife','struggle','war','warfare','barrage','brush','carnage','contention','engagement','fray','havoc','onset','onslaught','press','ravage','scrimmage','sortie','blitzkreig']\n",
    "politics= [\"politicize\", \"political affairs\", \"administration\", \"gandhi\", \"Modi\", \"politics\", \"politicians\", \"elections\", \"votes\", \"voters\", \"BJP\", \"Congress\", \"Supreme Court\",\"Nationalism\",\"polls\",\"poll\",\"voting\"]\n",
    "violence = [\"violence\", \"criminal violence\", \"criminal\", \"crime\", \"criminal party\", \"election violence\", \"electoral violence\",\"mob killing\", \"vigilante\", \"riots\", \"killing\" ]\n",
    "parties = ['bharatiya janata party', 'bjp', 'b.j.p.', 'bhaujan samaj party', 'bsp', 'b.s.p.', 'indian national congress', 'inc', 'i.n.c.', 'congress', 'nationalist congress party', 'ncp', 'n.c.p.','communist party of india','cpi','c.p.i.','aam aadmi party','aap','a.a.p.','samajwadi party','shiv sena','shs','rashtriya janta dal','rjd','r.j.d.']\n",
    "    \n",
    "eventTypes ={\n",
    "'riots':['riot'],\n",
    "'protests':['protest','march'], \n",
    "'civilianViolence':['rebel','violence','militia','war','rape','torture','kill','shoot','fight','murder','molest','attack','terror','battle','assault','blood']\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "count = 1\n",
    "\n",
    "for source, link in sources.items():\n",
    "    print(source)\n",
    "    count = 1\n",
    "    if 'rss' in link:\n",
    "        parsedFeed = fp.parse(link['rss'])\n",
    "        collected = {\n",
    "            'rss': link['rss'],\n",
    "            'link': link['link'],\n",
    "            'article': []\n",
    "        }\n",
    "        for feed in parsedFeed.entries:\n",
    "            \n",
    "            try:\n",
    "\n",
    "                articlesData ={}\n",
    "                articles = Article(feed['links'][0]['href'])\n",
    "                articles.download()\n",
    "                articles.parse()\n",
    "                articles.nlp()\n",
    "                strg = re.sub(r'[^\\w\\s]','',articles.text)\n",
    "                articleTokens = nlp(strg)\n",
    "\n",
    "                tokens =[]\n",
    "                for token in articleTokens:\n",
    "                    tokens.append(token.text.lower())\n",
    "\n",
    "                txt = str(articles.text).split()\n",
    "                flag = True\n",
    "                for i in txt:\n",
    "                    for j in events:\n",
    "                        if i==j:\n",
    "                            event_type = j\n",
    "                            flag = False\n",
    "                            break\n",
    "                        else:\n",
    "                            event_type = \"other\"\n",
    "                    if not flag:\n",
    "                        break\n",
    "                keyWords = articles.keywords\n",
    "\n",
    "                eventType = \"other\"\n",
    "                if(([a for a in keyWords if any(b.lower() in a.lower() for b in politics)])):\n",
    "                    print(articles.title)\n",
    "                    articlesData['title'] = articles.title\n",
    "\n",
    "                    article = nlp(articles.text)\n",
    "                    ###Summary\n",
    "                    sentences = [x for x in article.sents]\n",
    "                    sentences = list( map(str, sentences) )\n",
    "                    lxr = LexRank(articles.text)\n",
    "                    lxr.get_summary(sentences)\n",
    "                    summary = lxr.get_summary(sentences, summary_size=1, threshold=.1)\n",
    "                    summary = (\" \".join(summary))\n",
    "                    \n",
    "                    rawSummary = [preprocess(summary)]\n",
    "                    textVector = vector.transform(rawSummary)\n",
    "                    prediction = rf.predict(textVector)\n",
    "                    print(encoder.inverse_transform(prediction))\n",
    "                    articlesData['event'] = encoder.inverse_transform(prediction[0])\n",
    "\n",
    "                    dates = articleDateExtractor.extractArticlePublishedDate(feed['links'][0]['href'])\n",
    "                    localFormat = dates.strftime(\"%m/%d/%Y\")\n",
    "                    summaryDate = dates.strftime(\"On %d %b,\")\n",
    "                    articlesData['eventDate'] = localFormat\n",
    "                    articlesData['location'] = getLocation(articleTokens.ents, articles.text)\n",
    "                    orgs = set(keyWords).intersection(set(parties))\n",
    "\n",
    "                    if len(orgs) == 0:\n",
    "                        if(articlesData['event']==\"Riots\"):\n",
    "                            taggedOrg = \"Rioters\"\n",
    "                        elif(articlesData['event']==\"Protests\"):\n",
    "                            taggedOrg = \"Protestors\"\n",
    "                        else:\n",
    "                            taggedOrg = \"other\"\n",
    "                    else:\n",
    "                        taggedOrg=\"\"\n",
    "                        for org in orgs:\n",
    "                            taggedOrg+=org+\", \"\n",
    "                        taggedOrg=taggedOrg[:-2]\n",
    "\n",
    "                    articlesData['partiesInvolved'] = taggedOrg\n",
    "                    articlesData['source'] = source\n",
    "                    totalSummary = []\n",
    "                    totalSummary.append(summaryDate)\n",
    "                    totalSummary.append(summary)\n",
    "                    extractiveSummary  = (\" \".join(totalSummary))\n",
    "                    articlesData['summary'] = extractiveSummary\n",
    "                    articlesData['link'] = (feed['links'][0]['href'])\n",
    "                    collected['article'].append(articlesData)\n",
    "#                 else:\n",
    "#                     break\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "        collections['total'].append(collected)\n",
    "    else:\n",
    "        parsedFeed = newspaper.build(link['link'], memoize_articles = False)\n",
    "        collected = {\n",
    "            'link': link['link'],\n",
    "            'article': []\n",
    "        }\n",
    "        for feed in parsedFeed.articles:\n",
    "#             if count > 1000:\n",
    "#                 break\n",
    "            try:\n",
    "                feed.download()\n",
    "                feed.parse()\n",
    "                feed.nlp()\n",
    "                if feed.publish_date is not None:\n",
    "                    articlesData ={}\n",
    "                    \n",
    "                    strg = re.sub(r'[^\\w\\s]','',feed.text)\n",
    "                    articleTokens = nlp(strg)\n",
    "                    tokens =[]\n",
    "                    for token in articleTokens:\n",
    "                        tokens.append(token.text.lower())\n",
    "\n",
    "                        txt = str(feed.text).split()\n",
    "                        for i in txt:\n",
    "                            for j in events:\n",
    "                                if i==j:\n",
    "                                    event_type = j\n",
    "                                    break\n",
    "                                else:\n",
    "                                    event_type = \"other\"\n",
    "                    keyWords = feed.keywords\n",
    "                    if(([a for a in keyWords if any(b.lower() in a.lower() for b in politics)])):\n",
    "                        \n",
    "                        articlesData['title'] = feed.title\n",
    "                        article = nlp(feed.text)\n",
    "                        ###Summary\n",
    "                        sentences = [x for x in article.sents]\n",
    "                        sentences = list( map(str, sentences) )\n",
    "                        lxr = LexRank(feed.text)\n",
    "                        lxr.get_summary(sentences)\n",
    "                        summary = lxr.get_summary(sentences, summary_size=1, threshold=.1)\n",
    "                        summary = (\" \".join(summary))\n",
    "\n",
    "                        rawSummary = [preprocess(summary)]\n",
    "                        textVector = vector.transform(rawSummary)\n",
    "                        prediction = rf.predict(textVector)\n",
    "                        print(encoder.inverse_transform(prediction))\n",
    "                        articlesData['event'] = encoder.inverse_transform(prediction)\n",
    "                        \n",
    "#                         articlesData['event'] = getEventType(tokens)\n",
    "                        articlesData['eventDate'] = feed.publish_date.strftime('%m/%d/%Y')\n",
    "                        articlesData['location'] = getLocation(articleTokens.ents,feed.text)\n",
    "                        \n",
    "                        orgs = set(keyWords).intersection(set(parties))\n",
    "                        if len(orgs) == 0:\n",
    "                            if(articlesData['event']==\"Riots\"):\n",
    "                                taggedOrg = \"Rioters\"\n",
    "                            elif(articlesData['event']==\"Protests\"):\n",
    "                                taggedOrg = \"Protestors\"\n",
    "                            else:\n",
    "                                taggedOrg = \"other\"\n",
    "                        else:\n",
    "                            taggedOrg=\"\"\n",
    "                            for org in orgs:\n",
    "                                taggedOrg+=org+\", \"\n",
    "                            taggedOrg=taggedOrg[:-2]\n",
    "\n",
    "                        articlesData['partiesInvolved'] = taggedOrg\n",
    "                        articlesData['source'] = source\n",
    "                        totalSummary = []\n",
    "                        totalSummary.append(feed.publish_date.strftime(\"On %d %b,\"))\n",
    "                        totalSummary.append(summary)\n",
    "                        extractiveSummary  = (\" \".join(totalSummary))\n",
    "                        articlesData['summary'] = extractiveSummary\n",
    "                        articlesData['link'] = link\n",
    "                        collected['article'].append(articlesData)\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "        collections['total'].append(collected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rss': 'https://timesofindia.indiatimes.com/rssfeeds/-2128936835.cms',\n",
       "  'link': 'https://timesofindia.indiatimes.com/elections/news',\n",
       "  'article': []},\n",
       " {'rss': 'https://www.hindustantimes.com/rss/india/rssfeed.xml',\n",
       "  'link': 'https://www.hindustantimes.com/',\n",
       "  'article': []},\n",
       " {'rss': 'https://www.indiatoday.in/rss/1206578',\n",
       "  'link': 'https://indianexpress.com/?s=politics',\n",
       "  'article': []},\n",
       " {'rss': 'https://www.business-standard.com/rss/latest.rss',\n",
       "  'link': 'https://www.business-standard.com/lok-sabha-elections-2019',\n",
       "  'article': []}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections[\"total\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>link</th>\n",
       "      <th>rss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>https://timesofindia.indiatimes.com/elections/...</td>\n",
       "      <td>https://timesofindia.indiatimes.com/rssfeeds/-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>https://www.hindustantimes.com/</td>\n",
       "      <td>https://www.hindustantimes.com/rss/india/rssfe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>https://indianexpress.com/?s=politics</td>\n",
       "      <td>https://www.indiatoday.in/rss/1206578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>https://www.business-standard.com/lok-sabha-el...</td>\n",
       "      <td>https://www.business-standard.com/rss/latest.rss</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  article                                               link  \\\n",
       "0      []  https://timesofindia.indiatimes.com/elections/...   \n",
       "1      []                    https://www.hindustantimes.com/   \n",
       "2      []              https://indianexpress.com/?s=politics   \n",
       "3      []  https://www.business-standard.com/lok-sabha-el...   \n",
       "\n",
       "                                                 rss  \n",
       "0  https://timesofindia.indiatimes.com/rssfeeds/-...  \n",
       "1  https://www.hindustantimes.com/rss/india/rssfe...  \n",
       "2              https://www.indiatoday.in/rss/1206578  \n",
       "3   https://www.business-standard.com/rss/latest.rss  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame = json_normalize(collections['total'])\n",
    "rows = frame.shape[0]\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalDF = pd.DataFrame()\n",
    "for row in range(0,rows):\n",
    "    tempDF = pd.DataFrame()\n",
    "    tempDF = json_normalize(collections['total'][row]['article'])\n",
    "    totalDF = totalDF.append(tempDF, ignore_index = True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe that contains scraped news which has been classified and summarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "totalDF.to_excel(\"airData/india/testing_\"+now.strftime(\"%m%d%Y_%H%M%S\")+\".xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
